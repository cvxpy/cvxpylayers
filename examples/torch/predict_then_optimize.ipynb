{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict-then-Optimize: Decision-Focused Learning\n",
    "\n",
    "This notebook demonstrates **end-to-end predict-then-optimize** with CVXPYlayers â€” a neural network predicts optimization parameters, and a CvxpyLayer solves the downstream optimization. We compare:\n",
    "\n",
    "1. **Two-stage**: Train the predictor to minimize prediction MSE, then optimize\n",
    "2. **Decision-focused**: Train end-to-end to minimize *decision regret* (suboptimality of decisions)\n",
    "\n",
    "**Problem**: A neural net predicts resource costs from features, then a linear program allocates resources.\n",
    "\n",
    "$$\\min_x \\; \\hat{c}(\\text{features})^T x \\quad \\text{s.t.} \\; Ax \\leq b, \\; x \\geq 0$$\n",
    "\n",
    "**Key idea**: Minimizing prediction error doesn't minimize decision error. A cost that's wrong but leads to the same optimal decision is harmless, while a small error near a decision boundary can be catastrophic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from cvxpylayers.torch import CvxpyLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "Features map to true costs via a nonlinear function. Optimal decisions are computed using true costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_resources = 5   # decision variables\n",
    "n_features = 8    # input features\n",
    "n_constraints = 3 # resource constraints\n",
    "n_train = 200\n",
    "n_test = 100\n",
    "\n",
    "# Fixed constraint matrix and budget\n",
    "A_constr = np.random.rand(n_constraints, n_resources) + 0.5\n",
    "b_constr = np.ones(n_constraints) * 3.0\n",
    "\n",
    "# Generate features\n",
    "X_all = torch.randn(n_train + n_test, n_features, dtype=torch.float64)\n",
    "\n",
    "# True cost function: nonlinear mapping from features to costs\n",
    "W_true = torch.randn(n_features, n_resources, dtype=torch.float64) * 0.5\n",
    "\n",
    "def true_costs(X):\n",
    "    \"\"\"Nonlinear feature-to-cost mapping.\"\"\"\n",
    "    return torch.abs(X @ W_true) + 0.1  # ensure positive costs\n",
    "\n",
    "C_all = true_costs(X_all)\n",
    "\n",
    "# Split\n",
    "X_train, X_test = X_all[:n_train], X_all[n_train:]\n",
    "C_train, C_test = C_all[:n_train], C_all[n_train:]\n",
    "\n",
    "print(f\"Features: {n_features}, Resources: {n_resources}\")\n",
    "print(f\"Train: {n_train}, Test: {n_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LP and CvxpyLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = cp.Variable(n_resources, name=\"allocation\")\n",
    "c_param = cp.Parameter(n_resources, name=\"costs\")\n",
    "\n",
    "constraints = [\n",
    "    A_constr @ x_var <= b_constr,\n",
    "    x_var >= 0,\n",
    "]\n",
    "objective = cp.Minimize(c_param @ x_var)\n",
    "problem = cp.Problem(objective, constraints)\n",
    "assert problem.is_dpp()\n",
    "\n",
    "opt_layer = CvxpyLayer(problem, parameters=[c_param], variables=[x_var])\n",
    "print(\"LP layer created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prediction Network\n",
    "\n",
    "A small MLP maps features to predicted costs. We'll train two copies: one with MSE loss, one with decision loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, n_resources),\n",
    "            nn.Softplus(),  # ensure positive costs\n",
    "        )\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Two copies with same initialization\n",
    "torch.manual_seed(123)\n",
    "model_mse = CostPredictor()\n",
    "torch.manual_seed(123)\n",
    "model_dfl = CostPredictor()\n",
    "\n",
    "print(f\"Parameters per model: {sum(p.numel() for p in model_mse.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "**Two-stage** (MSE): minimize $\\|\\hat{c} - c_{\\text{true}}\\|^2$\n",
    "\n",
    "**Decision-focused** (DFL): minimize *regret* $= c_{\\text{true}}^T x(\\hat{c}) - c_{\\text{true}}^T x^\\star$, where $x(\\hat{c})$ is the decision from predicted costs and $x^\\star$ is the optimal decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "\n",
    "# Precompute optimal decisions under true costs\n",
    "x_stars_train = []\n",
    "for i in range(n_train):\n",
    "    (x_opt,) = opt_layer(C_train[i])\n",
    "    x_stars_train.append(x_opt.detach())\n",
    "x_stars_train = torch.stack(x_stars_train)\n",
    "\n",
    "opt_mse = torch.optim.Adam(model_mse.parameters(), lr=lr)\n",
    "opt_dfl = torch.optim.Adam(model_dfl.parameters(), lr=lr)\n",
    "\n",
    "mse_losses_hist = []\n",
    "dfl_losses_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Random batch\n",
    "    idx = torch.randperm(n_train)[:batch_size]\n",
    "    X_batch = X_train[idx]\n",
    "    C_batch = C_train[idx]\n",
    "    x_star_batch = x_stars_train[idx]\n",
    "\n",
    "    # --- Two-stage (MSE) ---\n",
    "    opt_mse.zero_grad()\n",
    "    c_pred_mse = model_mse(X_batch)\n",
    "    loss_mse = torch.mean((c_pred_mse - C_batch) ** 2)\n",
    "    loss_mse.backward()\n",
    "    opt_mse.step()\n",
    "    mse_losses_hist.append(loss_mse.item())\n",
    "\n",
    "    # --- Decision-focused ---\n",
    "    opt_dfl.zero_grad()\n",
    "    c_pred_dfl = model_dfl(X_batch)\n",
    "    \n",
    "    # Solve LP with predicted costs (one at a time for stability)\n",
    "    regrets = []\n",
    "    for j in range(len(idx)):\n",
    "        (x_pred,) = opt_layer(c_pred_dfl[j])\n",
    "        # Regret = true_cost @ predicted_decision - true_cost @ optimal_decision\n",
    "        regret = C_batch[j] @ x_pred - C_batch[j] @ x_star_batch[j]\n",
    "        regrets.append(regret)\n",
    "    loss_dfl = torch.mean(torch.stack(regrets))\n",
    "    loss_dfl.backward()\n",
    "    opt_dfl.step()\n",
    "    dfl_losses_hist.append(loss_dfl.item())\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | MSE loss: {loss_mse.item():.4f} | DFL regret: {loss_dfl.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate: Decision Quality on Test Set\n",
    "\n",
    "What matters is not prediction accuracy, but the quality of *decisions* made using the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, C_true):\n",
    "    \"\"\"Compute prediction MSE and decision regret on a dataset.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        c_pred = model(X)\n",
    "    pred_mse = torch.mean((c_pred - C_true) ** 2).item()\n",
    "\n",
    "    regrets = []\n",
    "    for i in range(len(X)):\n",
    "        (x_pred,) = opt_layer(c_pred[i])\n",
    "        (x_opt,) = opt_layer(C_true[i])\n",
    "        regret = (C_true[i] @ x_pred - C_true[i] @ x_opt).item()\n",
    "        regrets.append(regret)\n",
    "    avg_regret = np.mean(regrets)\n",
    "    return pred_mse, avg_regret, regrets\n",
    "\n",
    "mse_pred_err, mse_regret, mse_regrets = evaluate_model(model_mse, X_test, C_test)\n",
    "dfl_pred_err, dfl_regret, dfl_regrets = evaluate_model(model_dfl, X_test, C_test)\n",
    "\n",
    "print(f\"{'Method':<25} {'Prediction MSE':>15} {'Decision Regret':>15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Two-stage (MSE)':<25} {mse_pred_err:>15.4f} {mse_regret:>15.6f}\")\n",
    "print(f\"{'Decision-focused (DFL)':<25} {dfl_pred_err:>15.4f} {dfl_regret:>15.6f}\")\n",
    "print()\n",
    "print(\"Note: DFL may have higher prediction MSE but lower decision regret.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Training curves\n",
    "ax = axes[0]\n",
    "ax.plot(mse_losses_hist, label='Two-stage (MSE)', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('MSE Training Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(dfl_losses_hist, label='Decision-focused', color='orange', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Regret')\n",
    "ax.set_title('DFL Training Regret')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Regret comparison\n",
    "ax = axes[2]\n",
    "positions = [0, 1]\n",
    "ax.boxplot([mse_regrets, dfl_regrets], positions=positions, widths=0.5)\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(['Two-stage', 'Decision-focused'])\n",
    "ax.set_ylabel('Decision Regret')\n",
    "ax.set_title('Test Set Regret Distribution')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}