{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Optimization: Learning Uncertainty Sets\n",
    "\n",
    "This notebook demonstrates **robust optimization** with CVXPYlayers — designing solutions that perform well under worst-case uncertainty, and *learning* the right amount of robustness from data.\n",
    "\n",
    "**Problem**: Robust least-squares regression. Given uncertain data matrix $A$, we solve:\n",
    "\n",
    "$$\\min_x \\; \\|Ax - b\\|_2^2 + \\gamma \\cdot \\epsilon \\cdot \\|x\\|_2$$\n",
    "\n",
    "The regularization term $\\gamma \\cdot \\epsilon \\cdot \\|x\\|_2$ is the robust counterpart for spectral-norm bounded perturbations $\\|\\Delta\\| \\leq \\epsilon$. The uncertainty radius $\\epsilon$ is a learnable parameter trained to minimize out-of-sample prediction error.\n",
    "\n",
    "**Key idea**: Too little robustness ($\\epsilon \\approx 0$) overfits; too much robustness underperforms. CVXPYlayers lets us *learn* the right $\\epsilon$ by backpropagating through the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from cvxpylayers.torch import CvxpyLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data with Perturbations\n",
    "\n",
    "We create training data from a linear model $y = A_{\\text{true}} x_{\\text{true}} + \\text{noise}$, but the observed $A$ has measurement errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_features = 10\n",
    "n_train = 30\n",
    "n_val = 200\n",
    "noise_std = 0.3\n",
    "perturbation_std = 0.15  # measurement error in A\n",
    "\n",
    "# True model\n",
    "x_true = np.random.randn(n_features)\n",
    "x_true /= np.linalg.norm(x_true)\n",
    "\n",
    "# True (clean) data matrix\n",
    "A_true = np.random.randn(n_train, n_features)\n",
    "b_train = A_true @ x_true + noise_std * np.random.randn(n_train)\n",
    "\n",
    "# Observed (perturbed) training data\n",
    "A_train = A_true + perturbation_std * np.random.randn(n_train, n_features)\n",
    "\n",
    "# Validation data (clean A, noisy b)\n",
    "A_val = np.random.randn(n_val, n_features)\n",
    "b_val = A_val @ x_true + noise_std * np.random.randn(n_val)\n",
    "\n",
    "print(f\"Training: {n_train} samples, {n_features} features\")\n",
    "print(f\"Validation: {n_val} samples\")\n",
    "print(f\"Perturbation std: {perturbation_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Robust Least-Squares Layer\n",
    "\n",
    "The robust counterpart of least-squares with spectral norm uncertainty $\\|\\Delta\\| \\leq \\epsilon$ adds a regularization term $\\epsilon \\|x\\|_2$. This is equivalent to Tikhonov regularization, but with a principled interpretation as worst-case robustness.\n",
    "\n",
    "We parametrize the problem with:\n",
    "- `b_param` — the observed response vector\n",
    "- `epsilon` — the uncertainty radius (to be learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = cp.Variable(n_features, name=\"x\")\n",
    "b_param = cp.Parameter(n_train, name=\"b\")\n",
    "epsilon = cp.Parameter(nonneg=True, name=\"epsilon\")\n",
    "\n",
    "# A_train is fixed (not a parameter for simplicity)\n",
    "A_np = A_train.copy()\n",
    "\n",
    "# Robust counterpart: ||Ax - b||^2 + epsilon * ||x||_2\n",
    "objective = cp.Minimize(\n",
    "    cp.sum_squares(A_np @ x_var - b_param) + epsilon * cp.norm(x_var, 2)\n",
    ")\n",
    "problem = cp.Problem(objective)\n",
    "assert problem.is_dpp()\n",
    "\n",
    "layer = CvxpyLayer(problem, parameters=[b_param, epsilon], variables=[x_var])\n",
    "print(\"Robust least-squares layer created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve for Different Uncertainty Radii\n",
    "\n",
    "Sweep $\\epsilon$ to see how robustness affects training and validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tch = torch.tensor(b_train, dtype=torch.float64)\n",
    "A_val_tch = torch.tensor(A_val, dtype=torch.float64)\n",
    "b_val_tch = torch.tensor(b_val, dtype=torch.float64)\n",
    "A_train_tch = torch.tensor(A_train, dtype=torch.float64)\n",
    "\n",
    "eps_values = np.logspace(-2, 2, 30)\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "for eps_val in eps_values:\n",
    "    eps_tch = torch.tensor(eps_val, dtype=torch.float64)\n",
    "    (x_sol,) = layer(b_tch, eps_tch)\n",
    "    \n",
    "    train_err = torch.mean((A_train_tch @ x_sol - b_tch) ** 2).item()\n",
    "    val_err = torch.mean((A_val_tch @ x_sol - b_val_tch) ** 2).item()\n",
    "    train_errors.append(train_err)\n",
    "    val_errors.append(val_err)\n",
    "\n",
    "best_idx = np.argmin(val_errors)\n",
    "print(f\"Best epsilon (grid search): {eps_values[best_idx]:.4f}\")\n",
    "print(f\"Best validation MSE:        {val_errors[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Epsilon via SGD\n",
    "\n",
    "Instead of grid search, use gradient descent to find the optimal $\\epsilon$. CVXPYlayers differentiates through the solve, so we can directly minimize validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnable epsilon (log-parameterized for positivity)\n",
    "log_eps = torch.tensor(0.0, dtype=torch.float64, requires_grad=True)\n",
    "optimizer = torch.optim.Adam([log_eps], lr=0.1)\n",
    "\n",
    "losses = []\n",
    "eps_trajectory = []\n",
    "\n",
    "for step in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    eps_tch = torch.exp(log_eps)\n",
    "    (x_sol,) = layer(b_tch, eps_tch)\n",
    "    \n",
    "    # Minimize validation MSE\n",
    "    val_loss = torch.mean((A_val_tch @ x_sol - b_val_tch) ** 2)\n",
    "    val_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(val_loss.item())\n",
    "    eps_trajectory.append(eps_tch.item())\n",
    "\n",
    "learned_eps = torch.exp(log_eps).item()\n",
    "print(f\"Learned epsilon: {learned_eps:.4f}\")\n",
    "print(f\"Final validation MSE: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Robust vs. Nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Plot 1: Train/val error vs epsilon\n",
    "ax = axes[0]\n",
    "ax.semilogx(eps_values, train_errors, 'b-', label='Train MSE', linewidth=2)\n",
    "ax.semilogx(eps_values, val_errors, 'r-', label='Validation MSE', linewidth=2)\n",
    "ax.axvline(x=learned_eps, color='g', linestyle='--', label=f'Learned ε={learned_eps:.2f}')\n",
    "ax.axvline(x=eps_values[best_idx], color='orange', linestyle=':', label=f'Grid best ε={eps_values[best_idx]:.2f}')\n",
    "ax.set_xlabel('Uncertainty Radius (ε)')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Error vs. Robustness')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: SGD convergence\n",
    "ax = axes[1]\n",
    "ax.plot(losses, 'g-', linewidth=2)\n",
    "ax.set_xlabel('SGD Step')\n",
    "ax.set_ylabel('Validation MSE')\n",
    "ax.set_title('Learning Curve')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Epsilon trajectory\n",
    "ax = axes[2]\n",
    "ax.plot(eps_trajectory, 'g-', linewidth=2)\n",
    "ax.axhline(y=eps_values[best_idx], color='orange', linestyle=':', label='Grid search best')\n",
    "ax.set_xlabel('SGD Step')\n",
    "ax.set_ylabel('ε')\n",
    "ax.set_title('Learned ε Trajectory')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare nominal vs robust\n",
    "eps_nominal = torch.tensor(0.0, dtype=torch.float64)\n",
    "(x_nominal,) = layer(b_tch, eps_nominal)\n",
    "eps_robust = torch.tensor(learned_eps, dtype=torch.float64)\n",
    "(x_robust,) = layer(b_tch, eps_robust)\n",
    "\n",
    "nom_val_err = torch.mean((A_val_tch @ x_nominal - b_val_tch) ** 2).item()\n",
    "rob_val_err = torch.mean((A_val_tch @ x_robust - b_val_tch) ** 2).item()\n",
    "\n",
    "print(f\"Nominal (ε=0) validation MSE:   {nom_val_err:.4f}\")\n",
    "print(f\"Robust (ε={learned_eps:.2f}) validation MSE: {rob_val_err:.4f}\")\n",
    "print(f\"Improvement: {(nom_val_err - rob_val_err) / nom_val_err * 100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}